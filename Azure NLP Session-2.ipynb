{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Analytics Demo: https://aidemos.microsoft.com/text-analytics\n",
    "\n",
    "Pricing: https://azure.microsoft.com/en-gb/pricing/details/cognitive-services/text-analytics/#pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Ingopn01\\\\OneDrive - Ingram Micro\\\\Pictures\\\\Python\\\\2021\\\\Azure'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review1.txt\n",
      "Good Hotel and staff\n",
      "The Royal Hotel, London, UK\n",
      "3/2/2018\n",
      "Clean rooms, good service, great location near Buckingham Palace and Westminster Abbey, and so on. We thoroughly enjoyed our stay. The courtyard is very peaceful and we went to a restaurant which is part of the same group and is Indian ( West coast so plenty of fish) with a Michelin Star. We had the taster menu which was fabulous. The rooms were very well appointed with a kitchen, lounge, bedroom and enormous bathroom. Thoroughly recommended.\n",
      "\n",
      "Review2.txt\n",
      "Tired hotel with poor service\n",
      "The Royal Hotel, London, United Kingdom\n",
      "5/6/2018\n",
      "This is a old hotel (has been around since 1950's) and the room furnishings are average - becoming a bit old now and require changing. The internet didn't work and had to come to one of their office rooms to check in for my flight home. The website says it's close to the British Museum, but it's too far to walk.\n",
      "\n",
      "Review3.txt\n",
      "Tired hotel with poor service\n",
      "The Royal Hotel, London, United Kingdom\n",
      "5/6/2018\n",
      "This is a old hotel (has been around since 1950's) and the room furnishings are average - becoming a bit old now and require changing. The internet didn't work and had to come to one of their office rooms to check in for my flight home. The website says it's close to the British Museum, but it's too far to walk.\n",
      "\n",
      "Review4.txt\n",
      "Very noisy and rooms are tiny\n",
      "The Lombard Hotel, San Francisco, USA\n",
      "9/5/2018\n",
      "Hotel is located on Lombard street which is a very busy SIX lane street directly off the Golden Gate Bridge. Traffic from early morning until late at night especially on weekends. Noise would not be so bad if rooms were better insulated but they are not. Had to put cotton balls in my ears to be able to sleep--was too tired to enjoy the city the next day. Rooms are TINY. I picked the room because it had two queen size beds--but the room barely had space to fit them. With family of four in the room it was tight. With all that said, rooms are clean and they've made an effort to update them. The hotel is in Marina district with lots of good places to eat, within walking distance to Presidio. May be good hotel for young stay-up-late adults on a budget\n",
      "\n",
      "\n",
      "Review6.txt\n",
      "Saturn is the sixth planet from the Sun and the second-largest in the Solar System, after Jupiter. It is a gas giant with an average radius about nine times that of Earth.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Read the reviews in the /data/reviews folder\n",
    "reviews_folder = os.path.join('data', 'text', 'reviews')\n",
    "\n",
    "# Create a collection of reviews with id (file name) and text (contents) properties\n",
    "reviews = []\n",
    "for file_name in os.listdir(reviews_folder):\n",
    "    review_text = open(os.path.join(reviews_folder, file_name),encoding=\"utf8\").read()\n",
    "    review = {\"id\": file_name, \"text\": review_text}\n",
    "    reviews.append(review)\n",
    "\n",
    "for review_num in range(len(reviews)):\n",
    "    # print the review text\n",
    "    print('{}\\n{}\\n'.format(reviews[review_num]['id'], reviews[review_num]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Review 5 :Ganeshan loved cricket. After attending the English subject class when he rushed to the playground, due to the distance of playground he was always late. His old friends persuaded him to miss his class for next two days so that he could practice for an important match. His coach was also agree to help him by speaking to his class teacher, for this. But the coach ditched him and Ganeshan was taken to task for his absence of two days without prior school permission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use cognitive services at https://nlpsession.cognitiveservices.azure.com/ using key Your Key\n"
     ]
    }
   ],
   "source": [
    "cog_key = 'Your Key'\n",
    "cog_endpoint = 'https://nlpsession.cognitiveservices.azure.com/'\n",
    "\n",
    "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-ai-textanalytics in d:\\anaconda\\anaconda\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: six>=1.11.0 in d:\\anaconda\\anaconda\\lib\\site-packages (from azure-ai-textanalytics) (1.15.0)\n",
      "Requirement already satisfied: azure-common~=1.1 in d:\\anaconda\\anaconda\\lib\\site-packages (from azure-ai-textanalytics) (1.1.27)\n",
      "Requirement already satisfied: msrest>=0.6.21 in d:\\anaconda\\anaconda\\lib\\site-packages (from azure-ai-textanalytics) (0.6.21)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.14.0 in d:\\anaconda\\anaconda\\lib\\site-packages (from azure-ai-textanalytics) (1.17.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\anaconda\\lib\\site-packages (from msrest>=0.6.21->azure-ai-textanalytics) (2020.6.20)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in d:\\anaconda\\anaconda\\lib\\site-packages (from msrest>=0.6.21->azure-ai-textanalytics) (1.3.0)\n",
      "Requirement already satisfied: requests~=2.16 in d:\\anaconda\\anaconda\\lib\\site-packages (from msrest>=0.6.21->azure-ai-textanalytics) (2.24.0)\n",
      "Requirement already satisfied: isodate>=0.6.0 in d:\\anaconda\\anaconda\\lib\\site-packages (from msrest>=0.6.21->azure-ai-textanalytics) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\anaconda\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-ai-textanalytics) (3.1.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\anaconda\\anaconda\\lib\\site-packages (from requests~=2.16->msrest>=0.6.21->azure-ai-textanalytics) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\anaconda\\anaconda\\lib\\site-packages (from requests~=2.16->msrest>=0.6.21->azure-ai-textanalytics) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\anaconda\\anaconda\\lib\\site-packages (from requests~=2.16->msrest>=0.6.21->azure-ai-textanalytics) (3.0.4)\n",
      "Requirement already satisfied: azure-cognitiveservices-language-textanalytics in d:\\anaconda\\anaconda\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: azure-common~=1.1 in d:\\anaconda\\anaconda\\lib\\site-packages (from azure-cognitiveservices-language-textanalytics) (1.1.27)\n",
      "Requirement already satisfied: msrest>=0.5.0 in d:\\anaconda\\anaconda\\lib\\site-packages (from azure-cognitiveservices-language-textanalytics) (0.6.21)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in d:\\anaconda\\anaconda\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-language-textanalytics) (1.3.0)\n",
      "Requirement already satisfied: requests~=2.16 in d:\\anaconda\\anaconda\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-language-textanalytics) (2.24.0)\n",
      "Requirement already satisfied: isodate>=0.6.0 in d:\\anaconda\\anaconda\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-language-textanalytics) (0.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\anaconda\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-language-textanalytics) (2020.6.20)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\anaconda\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.0->azure-cognitiveservices-language-textanalytics) (3.1.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\anaconda\\anaconda\\lib\\site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-language-textanalytics) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\anaconda\\anaconda\\lib\\site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-language-textanalytics) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\anaconda\\anaconda\\lib\\site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-language-textanalytics) (1.25.11)\n",
      "Requirement already satisfied: six in d:\\anaconda\\anaconda\\lib\\site-packages (from isodate>=0.6.0->msrest>=0.5.0->azure-cognitiveservices-language-textanalytics) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-ai-textanalytics\n",
    "!pip install azure-cognitiveservices-language-textanalytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review1.txt\n",
      " - Language: English\n",
      " - Code: en\n",
      " - Score: 1.0\n",
      "\n",
      "Review2.txt\n",
      " - Language: English\n",
      " - Code: en\n",
      " - Score: 1.0\n",
      "\n",
      "Review3.txt\n",
      " - Language: English\n",
      " - Code: en\n",
      " - Score: 1.0\n",
      "\n",
      "Review4.txt\n",
      " - Language: English\n",
      " - Code: en\n",
      " - Score: 1.0\n",
      "\n",
      "Review6.txt\n",
      " - Language: English\n",
      " - Code: en\n",
      " - Score: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.cognitiveservices.language.textanalytics import TextAnalyticsClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "# Get a client for your text analytics cognitive service resource\n",
    "text_analytics_client = TextAnalyticsClient(endpoint=cog_endpoint,\n",
    "                                            credentials=CognitiveServicesCredentials(cog_key))\n",
    "\n",
    "# Analyze the reviews you read from the /data/reviews folder earlier\n",
    "language_analysis = text_analytics_client.detect_language(documents=reviews)\n",
    "\n",
    "# print detected language details for each review\n",
    "for review_num in range(len(reviews)):\n",
    "    # print the review id\n",
    "    print(reviews[review_num]['id'])\n",
    "\n",
    "    # Get the language details for this review\n",
    "    lang = language_analysis.documents[review_num].detected_languages[0]\n",
    "    print(' - Language: {}\\n - Code: {}\\n - Score: {}\\n'.format(lang.name, lang.iso6391_name, lang.score))\n",
    "\n",
    "    # Add the detected language code to the collection of reviews (so we can do further analysis)\n",
    "    reviews[review_num][\"language\"] = lang.iso6391_name\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Phrase extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review1.txt\n",
      "\n",
      "Key Phrases:\n",
      "\t Good Hotel\n",
      "\t good service\n",
      "\t Clean rooms\n",
      "\t Royal Hotel\n",
      "\t great location\n",
      "\t Buckingham Palace\n",
      "\t Westminster Abbey\n",
      "\t fish\n",
      "\t West coast\n",
      "\t lounge\n",
      "\t bedroom\n",
      "\t enormous bathroom\n",
      "\t group\n",
      "\t kitchen\n",
      "\t London\n",
      "\t UK\n",
      "\t taster menu\n",
      "\t Michelin Star\n",
      "\t staff\n",
      "\t courtyard\n",
      "\n",
      "\n",
      "Review2.txt\n",
      "\n",
      "Key Phrases:\n",
      "\t old hotel\n",
      "\t Royal Hotel\n",
      "\t Tired hotel\n",
      "\t London\n",
      "\t United Kingdom\n",
      "\t room furnishings\n",
      "\t poor service\n",
      "\t British Museum\n",
      "\t website\n",
      "\t office rooms\n",
      "\t flight home\n",
      "\t internet\n",
      "\n",
      "\n",
      "Review3.txt\n",
      "\n",
      "Key Phrases:\n",
      "\t old hotel\n",
      "\t Royal Hotel\n",
      "\t Tired hotel\n",
      "\t London\n",
      "\t United Kingdom\n",
      "\t room furnishings\n",
      "\t poor service\n",
      "\t British Museum\n",
      "\t website\n",
      "\t office rooms\n",
      "\t flight home\n",
      "\t internet\n",
      "\n",
      "\n",
      "Review4.txt\n",
      "\n",
      "Key Phrases:\n",
      "\t rooms\n",
      "\t good hotel\n",
      "\t Lombard Hotel\n",
      "\t Lombard street\n",
      "\t late adults\n",
      "\t good places\n",
      "\t lane street\n",
      "\t young stay\n",
      "\t night\n",
      "\t early morning\n",
      "\t Marina district\n",
      "\t San Francisco\n",
      "\t USA\n",
      "\t Golden Gate Bridge\n",
      "\t walking distance\n",
      "\t queen size beds\n",
      "\t ears\n",
      "\t Traffic\n",
      "\t cotton balls\n",
      "\t city\n",
      "\t Presidio\n",
      "\t weekends\n",
      "\t budget\n",
      "\t day\n",
      "\t effort\n",
      "\t Noise\n",
      "\t space\n",
      "\t family\n",
      "\n",
      "\n",
      "Review6.txt\n",
      "\n",
      "Key Phrases:\n",
      "\t average radius\n",
      "\t Solar System\n",
      "\t times\n",
      "\t gas giant\n",
      "\t Jupiter\n",
      "\t Earth\n",
      "\t Saturn\n",
      "\t planet\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Use the client and reviews you created in the previous code cell to get key phrases\n",
    "key_phrase_analysis = text_analytics_client.key_phrases(documents=reviews)\n",
    "\n",
    "# print key phrases for each review\n",
    "for review_num in range(len(reviews)):\n",
    "    # print the review id\n",
    "    print(reviews[review_num]['id'])\n",
    "\n",
    "    # Get the key phrases in this review\n",
    "    print('\\nKey Phrases:')\n",
    "    key_phrases = key_phrase_analysis.documents[review_num].key_phrases\n",
    "    # Print each key phrase\n",
    "    for key_phrase in key_phrases:\n",
    "        print('\\t', key_phrase)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-sentiment-analysis?tabs=version-3-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review1.txt : positive (0.9999973773956299)\n",
      "Review2.txt : negative (5.662441253662109e-07)\n",
      "Review3.txt : negative (5.662441253662109e-07)\n",
      "Review4.txt : negative (2.0623207092285156e-05)\n",
      "Review6.txt : positive (0.5)\n"
     ]
    }
   ],
   "source": [
    "# Use the client and reviews you created previously to get sentiment scores\n",
    "sentiment_analysis = text_analytics_client.sentiment(documents=reviews)\n",
    "\n",
    "# Print the results for each review\n",
    "for review_num in range(len(reviews)):\n",
    "\n",
    "    # Get the sentiment score for this review\n",
    "    sentiment_score = sentiment_analysis.documents[review_num].score\n",
    "\n",
    "    # classifiy 'positive' if more than 0.5, \n",
    "    if sentiment_score < 0.5:\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        sentiment = 'positive'\n",
    "\n",
    "    # print file name and sentiment\n",
    "    print('{} : {} ({})'.format(reviews[review_num]['id'], sentiment, sentiment_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review1.txt\n",
      " - Location: The Royal Hotel (https://en.wikipedia.org/wiki/The_Royal_Hotel)\n",
      " - Location: London (https://en.wikipedia.org/wiki/London)\n",
      " - DateTime: 3/2/2018 \n",
      " - Location: Buckingham Palace (https://en.wikipedia.org/wiki/Buckingham_Palace)\n",
      " - Location: Westminster Abbey (https://en.wikipedia.org/wiki/Westminster_Abbey)\n",
      " - Location: India (https://en.wikipedia.org/wiki/India)\n",
      " - Location: West Coast Main Line (https://en.wikipedia.org/wiki/West_Coast_Main_Line)\n",
      "Review2.txt\n",
      " - Location: The Royal Hotel (https://en.wikipedia.org/wiki/The_Royal_Hotel)\n",
      " - Location: London (https://en.wikipedia.org/wiki/London)\n",
      " - Location: London \n",
      " - Location: United Kingdom \n",
      " - DateTime: 5/6/2018 \n",
      " - DateTime: since 1950's \n",
      " - DateTime: now \n",
      " - Location: British Museum (https://en.wikipedia.org/wiki/British_Museum)\n",
      "Review3.txt\n",
      " - Location: The Royal Hotel (https://en.wikipedia.org/wiki/The_Royal_Hotel)\n",
      " - Location: London (https://en.wikipedia.org/wiki/London)\n",
      " - Location: London \n",
      " - Location: United Kingdom \n",
      " - DateTime: 5/6/2018 \n",
      " - DateTime: since 1950's \n",
      " - DateTime: now \n",
      " - Location: British Museum (https://en.wikipedia.org/wiki/British_Museum)\n",
      "Review4.txt\n",
      " - Location: Lombard, Illinois (https://en.wikipedia.org/wiki/Lombard,_Illinois)\n",
      " - Location: San Francisco (https://en.wikipedia.org/wiki/San_Francisco)\n",
      " - Location: Lombard Street (San Francisco) (https://en.wikipedia.org/wiki/Lombard_Street_(San_Francisco))\n",
      " - Location: Lombard \n",
      " - Location: Golden Gate Bridge (https://en.wikipedia.org/wiki/Golden_Gate_Bridge)\n",
      " - DateTime: from early morning \n",
      " - DateTime: night \n",
      " - DateTime: the next day \n",
      " - Location: Marina \n",
      " - Location: Marina District, San Francisco (https://en.wikipedia.org/wiki/Marina_District,_San_Francisco)\n",
      " - Location: Presidio of San Francisco (https://en.wikipedia.org/wiki/Presidio_of_San_Francisco)\n",
      "Review6.txt\n",
      " - Location: Jupiter \n",
      " - Location: Earth \n"
     ]
    }
   ],
   "source": [
    "# Use the client and reviews you created previously to get named entities\n",
    "entity_analysis = text_analytics_client.entities(documents=reviews)\n",
    "\n",
    "# Print the results for each review\n",
    "for review_num in range(len(reviews)):\n",
    "    print(reviews[review_num]['id'])\n",
    "    # Get the named entitites in this review\n",
    "    entities = entity_analysis.documents[review_num].entities\n",
    "    for entity in entities:\n",
    "        # Only print datetime or location entitites\n",
    "        if entity.type in ['DateTime','Location']:\n",
    "            link = '(' + entity.wikipedia_url + ')' if entity.wikipedia_id is not None else ''\n",
    "            print(' - {}: {} {}'.format(entity.type, entity.name, link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Understanding (LUIS)\n",
    "\n",
    "A natural language understanding (NLU) AI service that allows users to interact with your applications, bots and IoT devices by using natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Detection Demo: https://aidemos.microsoft.com/luis/demo\n",
    "\n",
    "LUIS: https://azure.microsoft.com/en-in/services/cognitive-services/language-understanding-intelligent-service/#overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating Text\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/translator/language-support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use cognitive services in centralindia using key Your Key\n"
     ]
    }
   ],
   "source": [
    "cog_key = 'Your Key'\n",
    "cog_location = 'centralindia'\n",
    "\n",
    "print('Ready to use cognitive services in {} using key {}'.format(cog_location, cog_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello -> Bonjour\n"
     ]
    }
   ],
   "source": [
    "# Create a function that makes a REST request to the Text Translation service\n",
    "def translate_text(cog_location, cog_key, text, to_lang='fr', from_lang='en'):\n",
    "    import requests, uuid, json\n",
    "\n",
    "    # Create the URL for the Text Translator service REST request\n",
    "    path = 'https://api.cognitive.microsofttranslator.com/translate?api-version=3.0'\n",
    "    params = '&from={}&to={}'.format(from_lang, to_lang)\n",
    "    constructed_url = path + params\n",
    "\n",
    "    # Prepare the request headers with Cognitive Services resource key and region\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': cog_key,\n",
    "        'Ocp-Apim-Subscription-Region':cog_location,\n",
    "        'Content-type': 'application/json',\n",
    "        'X-ClientTraceId': str(uuid.uuid4())\n",
    "    }\n",
    "\n",
    "    # Add the text to be translated to the body\n",
    "    body = [{\n",
    "        'text': text\n",
    "    }]\n",
    "\n",
    "    # Get the translation\n",
    "    request = requests.post(constructed_url, headers=headers, json=body)\n",
    "    response = request.json()\n",
    "    return response[0][\"translations\"][0][\"text\"]\n",
    "\n",
    "\n",
    "# Test the function\n",
    "text_to_translate = \"Hello\"\n",
    "\n",
    "translation = translate_text(cog_location, cog_key, text_to_translate, to_lang='fr', from_lang='en')\n",
    "print('{} -> {}'.format(text_to_translate,translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello -> Ciao\n"
     ]
    }
   ],
   "source": [
    "text_to_translate = \"Hello\"\n",
    "\n",
    "translation = translate_text(cog_location, cog_key, text_to_translate, to_lang='it-IT', from_lang='en-GB')\n",
    "print('{} -> {}'.format(text_to_translate,translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello -> 你好\n"
     ]
    }
   ],
   "source": [
    "text_to_translate = \"Hello\"\n",
    "\n",
    "translation = translate_text(cog_location, cog_key, text_to_translate, to_lang='zh-CN', from_lang='en-US')\n",
    "print('{} -> {}'.format(text_to_translate,translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "गनेशन क्रिकेट का शौकीन था । अंग्रेजी कक्षा के बाद जब वह खेल के मैदान के लिए दौड़ता था । खेल का मैदान दूर होने की वजह उसे हमेशा देर हो जाती थी । उसके पुराने मित्रों ने उसे राजी कर लिया कि वह अगले दो दिन अपनी कक्षा में ना जाये ताकि वह महत्वपूर्ण मैच का अभ्यास कर ले । उसके प्रशिक्षक भी उसकी सहायता को सहमत हो गये कि वह कक्षा अध्यापक को बोल देंगे । लेकिन प्रशिक्षक ने उसे धोखा दिया और गनेशन को बिना पूर्व अनुमति के दो दिन अनुपस्थित रहने के लिए दण्ड दिया गया । -> Ganesan was fond of cricket. After the English class when he ran to the playground. He was always late because the playground was far away. His old friends persuaded him not to go to his class for the next two days so that he could practice the important match. His trainers also agreed to help him speak to the class teacher. But the trainer cheated him and Ganesan was punished for two days absent without prior permission.\n"
     ]
    }
   ],
   "source": [
    "text_to_translate = \"गनेशन क्रिकेट का शौकीन था । अंग्रेजी कक्षा के बाद जब वह खेल के मैदान के लिए दौड़ता था । खेल का मैदान दूर होने की वजह उसे हमेशा देर हो जाती थी । उसके पुराने मित्रों ने उसे राजी कर लिया कि वह अगले दो दिन अपनी कक्षा में ना जाये ताकि वह महत्वपूर्ण मैच का अभ्यास कर ले । उसके प्रशिक्षक भी उसकी सहायता को सहमत हो गये कि वह कक्षा अध्यापक को बोल देंगे । लेकिन प्रशिक्षक ने उसे धोखा दिया और गनेशन को बिना पूर्व अनुमति के दो दिन अनुपस्थित रहने के लिए दण्ड दिया गया ।\"\n",
    "\n",
    "translation = translate_text(cog_location, cog_key, text_to_translate, to_lang='en-US', from_lang='hi')\n",
    "print('{} -> {}'.format(text_to_translate,translation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving Forward: \n",
    "https://gentle-pebble-08cfc3110.azurestaticapps.net/#about\n",
    "\n",
    "https://vivekraja98.medium.com/literature-text-translation-audio-synthesis-using-microsoft-azure-cognitive-services-5e35add0c79e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Returns key phrases from the text in a comma-separated list\n",
    "(text) => let\n",
    "    apikey      = \"YOUR_API_KEY_HERE\",\n",
    "    endpoint    = \"https://<your-custom-subdomain>.cognitiveservices.azure.com/text/analytics\" & \"/v3.0/keyPhrases\",\n",
    "    jsontext    = Text.FromBinary(Json.FromValue(Text.Start(Text.Trim(text), 5000))),\n",
    "    jsonbody    = \"{ documents: [ { language: \"\"en\"\", id: \"\"0\"\", text: \" & jsontext & \" } ] }\",\n",
    "    bytesbody   = Text.ToBinary(jsonbody),\n",
    "    headers     = [#\"Ocp-Apim-Subscription-Key\" = apikey],\n",
    "    bytesresp   = Web.Contents(endpoint, [Headers=headers, Content=bytesbody]),\n",
    "    jsonresp    = Json.Document(bytesresp),\n",
    "    keyphrases  = Text.Lower(Text.Combine(jsonresp[documents]{0}[keyPhrases], \", \"))\n",
    "in  keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Returns the sentiment label of the text, for example, positive, negative or mixed.\n",
    "(text) => let\n",
    "    apikey = \"YOUR_API_KEY_HERE\",\n",
    "    endpoint = \"<your-custom-subdomain>.cognitiveservices.azure.com\" & \"/text/analytics/v3.1/sentiment\",\n",
    "    jsontext = Text.FromBinary(Json.FromValue(Text.Start(Text.Trim(text), 5000))),\n",
    "    jsonbody = \"{ documents: [ { language: \"\"en\"\", id: \"\"0\"\", text: \" & jsontext & \" } ] }\",\n",
    "    bytesbody = Text.ToBinary(jsonbody),\n",
    "    headers = [#\"Ocp-Apim-Subscription-Key\" = apikey],\n",
    "    bytesresp = Web.Contents(endpoint, [Headers=headers, Content=bytesbody]),\n",
    "    jsonresp = Json.Document(bytesresp),\n",
    "    sentiment   = jsonresp[documents]{0}[sentiment] \n",
    "    in sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Returns the two-letter language code (for example, 'en' for English) of the text\n",
    "(text) => let\n",
    "    apikey      = \"YOUR_API_KEY_HERE\",\n",
    "    endpoint    = \"https://<your-custom-subdomain>.cognitiveservices.azure.com\" & \"/text/analytics/v3.1/languages\",\n",
    "    jsontext    = Text.FromBinary(Json.FromValue(Text.Start(Text.Trim(text), 5000))),\n",
    "    jsonbody    = \"{ documents: [ { id: \"\"0\"\", text: \" & jsontext & \" } ] }\",\n",
    "    bytesbody   = Text.ToBinary(jsonbody),\n",
    "    headers     = [#\"Ocp-Apim-Subscription-Key\" = apikey],\n",
    "    bytesresp   = Web.Contents(endpoint, [Headers=headers, Content=bytesbody]),\n",
    "    jsonresp    = Json.Document(bytesresp),\n",
    "    language    = jsonresp [documents]{0}[detectedLanguage] [iso6391Name] in language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Returns the name (for example, 'English') of the language in which the text is written\n",
    "(text) => let\n",
    "    apikey      = \"YOUR_API_KEY_HERE\",\n",
    "    endpoint    = \"https://<your-custom-subdomain>.cognitiveservices.azure.com\" & \"/text/analytics/v3.1/languages\",\n",
    "    jsontext    = Text.FromBinary(Json.FromValue(Text.Start(Text.Trim(text), 5000))),\n",
    "    jsonbody    = \"{ documents: [ { id: \"\"0\"\", text: \" & jsontext & \" } ] }\",\n",
    "    bytesbody   = Text.ToBinary(jsonbody),\n",
    "    headers     = [#\"Ocp-Apim-Subscription-Key\" = apikey],\n",
    "    bytesresp   = Web.Contents(endpoint, [Headers=headers, Content=bytesbody]),\n",
    "    jsonresp    = Json.Document(bytesresp),\n",
    "    language    jsonresp [documents]{0}[detectedLanguage] [iso6391Name] in language"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
